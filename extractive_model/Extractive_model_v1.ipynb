{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/20765011/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import lzma\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1'>1. Pre-process X_train</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>token_ops</th>\n",
       "      <th>token_heads</th>\n",
       "      <th>op counts</th>\n",
       "      <th>head counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Walker , J . ##SENT## The grievance alleged by the plaintiff is that the defendant failed to place a refrigerating car for him at Wrightsboro by 2 :30 o ’clock p . m . on 16 Hay , 1918 , to receiv...</td>\n",
       "      <td>D . K . FUTCH v . ATLANTIC COAST LINE RAILROAD COMPANY . ##SENT## (Filed 15 October , 1919 .) ##SENT## 1 . ##SENT## Carriers of Goods — Placing of Cars — Understanding of Agent — Instructions — Ra...</td>\n",
       "      <td>1347</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Hoke , J . ##SENT## On the hearing it appeared that C . G . Bailey and others , executors of the last will and testament of W . A . Bailey , deceased , intended presently to make sale of a tract o...</td>\n",
       "      <td>MRS . SUSANNA WILLIAMS v . C . G . BAILEY , B . R . BAILEY et al ., Executors of W . R . BAILEY . ##SENT## (Filed 3 December , 1919 .) ##SENT## 1 . ##SENT## Deeds and Conveyances — Descriptions —R...</td>\n",
       "      <td>1649</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>HoKE , J . ##SENT## The facts pertinent to the inquiry and showing the action of the Superior Court thereon are very satisfactorily stated in the appellant ’s brief filed in the cause , and are as...</td>\n",
       "      <td>J . DICKSON McLEAN , Commissioner , EUGENE BOND , VICTOR BOND , ALLEN BOND , R . S . BOND , Executor and Trustee , Etc ., and W . LENNON , Guardian , Etc ., v . S . F . CALDWELL . ##SENT## (Filed ...</td>\n",
       "      <td>1229</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>Beown , J . ##SENT## Defendant was convicted at tbe June Term , 1919 , of Guilford County Superior Court , of bigamous cohabitation , under Rev ., 3361 , as amended by chapter 26 , Public Laws 191...</td>\n",
       "      <td>STATE v . JOHN W . MOON . ##SENT## (Filed 5 November , 1919 .) ##SENT## 1 . ##SENT## Statutes — Amendments —Effect. ##SENT## The effect of an amendment to a statute is to incorporate the old statu...</td>\n",
       "      <td>839</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>ClaRK , C . J . ##SENT## There is no exception to evidence or the charge . ##SENT## The sole assignment of error is the refusal of the motion to sever . ##SENT## From S . v . Smith , 24 N . C ., 4...</td>\n",
       "      <td>STATE v . ASHLEY SOUTHERLAND . ##SENT## (Filed 24 September , 1919 .) ##SENT## Indictments — Severance —Motions—Murder—Different Defenses — Conspiracy . ##SENT## Upon a motion for a severance unde...</td>\n",
       "      <td>1318</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           3   \n",
       "1           4   \n",
       "2           5   \n",
       "3           9   \n",
       "4          10   \n",
       "\n",
       "                                                                                                                                                                                                 token_ops  \\\n",
       "0  Walker , J . ##SENT## The grievance alleged by the plaintiff is that the defendant failed to place a refrigerating car for him at Wrightsboro by 2 :30 o ’clock p . m . on 16 Hay , 1918 , to receiv...   \n",
       "1  Hoke , J . ##SENT## On the hearing it appeared that C . G . Bailey and others , executors of the last will and testament of W . A . Bailey , deceased , intended presently to make sale of a tract o...   \n",
       "2  HoKE , J . ##SENT## The facts pertinent to the inquiry and showing the action of the Superior Court thereon are very satisfactorily stated in the appellant ’s brief filed in the cause , and are as...   \n",
       "3  Beown , J . ##SENT## Defendant was convicted at tbe June Term , 1919 , of Guilford County Superior Court , of bigamous cohabitation , under Rev ., 3361 , as amended by chapter 26 , Public Laws 191...   \n",
       "4  ClaRK , C . J . ##SENT## There is no exception to evidence or the charge . ##SENT## The sole assignment of error is the refusal of the motion to sever . ##SENT## From S . v . Smith , 24 N . C ., 4...   \n",
       "\n",
       "                                                                                                                                                                                               token_heads  \\\n",
       "0  D . K . FUTCH v . ATLANTIC COAST LINE RAILROAD COMPANY . ##SENT## (Filed 15 October , 1919 .) ##SENT## 1 . ##SENT## Carriers of Goods — Placing of Cars — Understanding of Agent — Instructions — Ra...   \n",
       "1  MRS . SUSANNA WILLIAMS v . C . G . BAILEY , B . R . BAILEY et al ., Executors of W . R . BAILEY . ##SENT## (Filed 3 December , 1919 .) ##SENT## 1 . ##SENT## Deeds and Conveyances — Descriptions —R...   \n",
       "2  J . DICKSON McLEAN , Commissioner , EUGENE BOND , VICTOR BOND , ALLEN BOND , R . S . BOND , Executor and Trustee , Etc ., and W . LENNON , Guardian , Etc ., v . S . F . CALDWELL . ##SENT## (Filed ...   \n",
       "3  STATE v . JOHN W . MOON . ##SENT## (Filed 5 November , 1919 .) ##SENT## 1 . ##SENT## Statutes — Amendments —Effect. ##SENT## The effect of an amendment to a statute is to incorporate the old statu...   \n",
       "4  STATE v . ASHLEY SOUTHERLAND . ##SENT## (Filed 24 September , 1919 .) ##SENT## Indictments — Severance —Motions—Murder—Different Defenses — Conspiracy . ##SENT## Upon a motion for a severance unde...   \n",
       "\n",
       "   op counts  head counts  \n",
       "0       1347          361  \n",
       "1       1649          361  \n",
       "2       1229          319  \n",
       "3        839          290  \n",
       "4       1318          303  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_small.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't need to get embeddings for headnote tokens, because our\n",
    "# RNN will only extract sentences from the opinion body text;\n",
    "# thus, our model never needs to \"read\" the headnotes\n",
    "all_text = df['token_ops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 1.34 s, total: 21.1 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_text = []\n",
    "for doc in df['token_ops']:\n",
    "    x = doc.replace(\"##SENT##\", \"\")\n",
    "    x_tok = nltk.tokenize.WordPunctTokenizer().tokenize(x)\n",
    "\n",
    "    all_text.extend(x_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arry = np.array([np.array(xi) for xi in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.unique(all_text)\n",
    "n_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 139335\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size: {}\".format(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = ['_PADDING_'] + list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 389 ms, sys: 0 ns, total: 389 ms\n",
      "Wall time: 387 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent_lengths = []\n",
    "for doc in df['token_ops']:\n",
    "    sents_in_doc = doc.split('##SENT##')\n",
    "    sent_lengths.append(len(sents_in_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "# max words in a sentence\n",
    "# Use this size for maxpooling layer\n",
    "max_sent_len = max(sent_lengths)\n",
    "print(max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for word index (vocabulary)\n",
    "word2idx = dict(zip(word_index, range(n_words+1)))\n",
    "idx2word = dict(zip(range(n_words+1), word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.7 s, sys: 6.6 ms, total: 47.7 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert to numeric using word2idx and add padding\n",
    "\n",
    "X = []\n",
    "for doc in df['token_ops']:\n",
    "    sents_in_doc = doc.split('##SENT##')\n",
    "    \n",
    "    mod_doc = []\n",
    "    for sent in sents_in_doc:\n",
    "        \n",
    "        mod_sent=[]\n",
    "        x_tokens = nltk.tokenize.WordPunctTokenizer().tokenize(sent)\n",
    "        # Convert tokens in a sentence to index numbers\n",
    "        for token in x_tokens:\n",
    "            mod_sent.append(word2idx[token])\n",
    "        mod_doc.append(mod_sent)\n",
    "    X.append(pad_sequences(mod_doc, maxlen=max_sent_len, padding='post', value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240,)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(([], X[0][1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for doc in X:\n",
    "    concat_doc = []\n",
    "    for sent in doc:\n",
    "        concat_doc.extend(sent)\n",
    "    X_train.append(concat_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length, by words: 57600\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = []\n",
    "for doc in X_train:\n",
    "    doc_lengths.append(len(doc))\n",
    "max_doc_len = max(doc_lengths)\n",
    "print(\"Max document length, by words: {}\".format(max_doc_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_final = pad_sequences(X_train, maxlen=max_doc_len, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24603, 57600)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2'>2. Pre-process Y_train labels</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unproc = []\n",
    "rouge_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[\"oracle_batch1.txt\", \"oracle_batch2.txt\", \"oracle_batch3.txt\",\n",
    "      \"oracle_batch4.txt\", \"oracle_batch6.txt\",\n",
    "      \"oracle_batch7.txt\", \"oracle_batch8.txt\", \"oracle_batch9.txt\",\n",
    "      \"oracle_batch10.txt\", \"oracle_batch11.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6633678/finding-words-after-keyword-in-python\n",
    "\n",
    "for file in files:\n",
    "    f = open(file, \"r\")\n",
    "    for line in f:\n",
    "        y_tup, split, rouge_score = line.partition('\\t')\n",
    "        rouge_score = rouge_score.strip('\\n')\n",
    "        y_unproc.append(y_tup)\n",
    "        rouge_scores.append(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_unproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy X_train, based off of indices of my generated y labels\n",
    "X_tr_toy = np.concatenate((X_tr_final[:1900], X_tr_final[2900:7000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3'>3. Small Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GloVE word embeddings\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "## I used wikipedia 2014+ Gigaword\n",
    "# Extract word vectors\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_doc_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 57600)]           0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 57600, 100)        13933700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 57600, 200)        121200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 240, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 240, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 240, 57600)        11577600  \n",
      "=================================================================\n",
      "Total params: 25,813,700\n",
      "Trainable params: 11,880,000\n",
      "Non-trainable params: 13,933,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# architecture inspired by SummaRunner \n",
    "# https://github.com/hpzhao/SummaRuNNer/blob/master/models/RNN_RNN.py\n",
    "n_units=100\n",
    "optimizer = \"adam\"\n",
    "loss = \"binary_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "seq_input = Input(shape=(max_doc_len,))\n",
    "embedded_seq = embedding_layer(seq_input)\n",
    "# Word-level GRU\n",
    "x = Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True))(embedded_seq)\n",
    "x = tf.keras.layers.MaxPool1D(pool_size = max_sent_len, padding='same')(x)\n",
    "\n",
    "# Sentence-level GRU after maxpooling all words in a sentence\n",
    "x = Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True))(x)\n",
    "\n",
    "# Classification at the sentence level\n",
    "output = Dense(max_doc_len, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=seq_input, outputs=output) \n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unproc = np.array(y_unproc)\n",
    "\n",
    "indices = np.arange(X_tr_toy.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_tr_toy = X_tr_toy[indices]\n",
    "y_unproc = y_unproc[indices]\n",
    "\n",
    "x_train = X_tr_toy[600:]\n",
    "y_train = y_unproc[600:]\n",
    "x_val = X_tr_toy[:600]\n",
    "y_val = y_unproc[:600]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (5400, 1) was passed for an output of shape (None, 240, 57600) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (5400, 1) was passed for an output of shape (None, 240, 57600) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train model\n",
    "verbose = 1\n",
    "\n",
    "callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=4)\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5,#epochs, \n",
    "                    validation_data=(x_val, y_val), verbose=verbose,\n",
    "                    shuffle=True,\n",
    "                    callbacks= [callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
