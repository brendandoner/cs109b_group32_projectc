{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note to self: matched DFs using true_df from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/20765011/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1. Pre-process X_train</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_df.csv can be found in this zip file that I generated for paralelling pre-processing:\n",
    "# https://drive.google.com/file/d/1yDXjd7seRCp3_YZDHky3utDVtBdIsSYL/view?usp=sharing\n",
    "df = pd.read_csv(\"true_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust df to match the y_train that we pre-processed by batching\n",
    "df_fixed = np.concatenate((df.iloc[900:1900],df.iloc[6000:6500],df.iloc[7000:9000], df.iloc[12000:16000], df.iloc[18000:24000]))\n",
    "df = pd.DataFrame(df_fixed, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>token_ops</th>\n",
       "      <th>token_heads</th>\n",
       "      <th>op counts</th>\n",
       "      <th>head counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2370</td>\n",
       "      <td>Taxxor , Chief -Justice. ##SENT## It is equally reasonable and correct , that a man shall not recover a recompense for an injury received by liis own consent •, but the rule must necessarily be re...</td>\n",
       "      <td>Stout v . Wren . ##SENT## From Randolph . ##SENT## A man shall not recover a recompense for an injury received by liis own consent , provided the act from which the injury is received be lawful : ...</td>\n",
       "      <td>521</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2372</td>\n",
       "      <td>Tanxor , Chief -Justice, delivered the opinion of the Court : The special verdict is framed with a view to obtain the judgment of the Court upon the question , whether the Defendant committed a nu...</td>\n",
       "      <td>State v . M ’Carson. ##SENT## tFrom Buncombe . ##SENT## The Defendant was indicted for having erected a gate across a public road . ##SENT## The defence résted on several acts of Assembly , the vi...</td>\n",
       "      <td>1107</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2380</td>\n",
       "      <td>Ruffin , Judge . ##SENT## — A very evasive answer , in this case , raises a strong suspicion in say mind that the Deiendant was a partner , which is fully confirmed by the proofs . ##SENT## )i sho...</td>\n",
       "      <td>Alvia Spear v . Bezaleel Gillet , From Wake . ##SENT## Where , upon a contract by copartners , made in Virginia , the bond oí one was taken to secure the partnership debt , it was held , that if b...</td>\n",
       "      <td>1195</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2386</td>\n",
       "      <td>Hall , Judge . ##SENT## The binds in question were part of a tract originally granted to Henry Cossart , from whom they descended to Christian Frederic Cossart , hi » son and heir at law . ##SENT#...</td>\n",
       "      <td>Christian L . Benzein et al . ##SENT## v . Jesse Robenett et al . ##SENT## From Wilkes . ##SENT## On the hearing of an original bill , in the nature of a supplemental bill and bill of revivor , de...</td>\n",
       "      <td>1738</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2388</td>\n",
       "      <td>Hall , Judge . ##SENT## — The Plaintiff does not call upon the Court f &lt; t - its assistance to supply any defect , or rectify a mistake in the voluntary deed of gift , which is the subject of the ...</td>\n",
       "      <td>Barden Tolar v . Nehemiah Tolar , From Wayne . ##SENT## If a voluntary deed , fairly obtained , is destroyed bv the donor before registration , a Court of Equity \\yill compel him to convey the sam...</td>\n",
       "      <td>1167</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  \\\n",
       "0       2370   \n",
       "1       2372   \n",
       "2       2380   \n",
       "3       2386   \n",
       "4       2388   \n",
       "\n",
       "                                                                                                                                                                                                 token_ops  \\\n",
       "0  Taxxor , Chief -Justice. ##SENT## It is equally reasonable and correct , that a man shall not recover a recompense for an injury received by liis own consent •, but the rule must necessarily be re...   \n",
       "1  Tanxor , Chief -Justice, delivered the opinion of the Court : The special verdict is framed with a view to obtain the judgment of the Court upon the question , whether the Defendant committed a nu...   \n",
       "2  Ruffin , Judge . ##SENT## — A very evasive answer , in this case , raises a strong suspicion in say mind that the Deiendant was a partner , which is fully confirmed by the proofs . ##SENT## )i sho...   \n",
       "3  Hall , Judge . ##SENT## The binds in question were part of a tract originally granted to Henry Cossart , from whom they descended to Christian Frederic Cossart , hi » son and heir at law . ##SENT#...   \n",
       "4  Hall , Judge . ##SENT## — The Plaintiff does not call upon the Court f < t - its assistance to supply any defect , or rectify a mistake in the voluntary deed of gift , which is the subject of the ...   \n",
       "\n",
       "                                                                                                                                                                                               token_heads  \\\n",
       "0  Stout v . Wren . ##SENT## From Randolph . ##SENT## A man shall not recover a recompense for an injury received by liis own consent , provided the act from which the injury is received be lawful : ...   \n",
       "1  State v . M ’Carson. ##SENT## tFrom Buncombe . ##SENT## The Defendant was indicted for having erected a gate across a public road . ##SENT## The defence résted on several acts of Assembly , the vi...   \n",
       "2  Alvia Spear v . Bezaleel Gillet , From Wake . ##SENT## Where , upon a contract by copartners , made in Virginia , the bond oí one was taken to secure the partnership debt , it was held , that if b...   \n",
       "3  Christian L . Benzein et al . ##SENT## v . Jesse Robenett et al . ##SENT## From Wilkes . ##SENT## On the hearing of an original bill , in the nature of a supplemental bill and bill of revivor , de...   \n",
       "4  Barden Tolar v . Nehemiah Tolar , From Wayne . ##SENT## If a voluntary deed , fairly obtained , is destroyed bv the donor before registration , a Court of Equity \\yill compel him to convey the sam...   \n",
       "\n",
       "  op counts head counts  \n",
       "0       521         220  \n",
       "1      1107          56  \n",
       "2      1195         397  \n",
       "3      1738         378  \n",
       "4      1167         370  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='a'>a. Generate vocabulary via tokenization for embeddings</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't need to get embeddings for headnote tokens, because our\n",
    "# RNN will only extract sentences from the opinion body text;\n",
    "# thus, my model never needs to \"read\" the headnotes\n",
    "all_text = df['token_ops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaner removes numbers and punctuation, as they are probably not needed\n",
    "# for the model to evaluate the IMPORTANCE of a sentence (not meaning)\n",
    "\n",
    "def txt_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    \n",
    "    #added line to remove parantheses\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString) # remove '\"'\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    \n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 s, sys: 897 ms, total: 27.7 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "# all_text is all the text, tokenized for generating the vocabulary, which we need for embedding\n",
    "# df_cleaned is the cleaned dataframe with punctuations/numbers removed\n",
    "%%time\n",
    "all_text = []\n",
    "df_cleaned = []\n",
    "\n",
    "for doc in df['token_ops']:\n",
    "    x = doc.replace(\"##SENT##\", \"newsenthere\")\n",
    "    x = txt_cleaner(x)\n",
    "    df_cleaned.append(x)\n",
    "    \n",
    "    x_tok = nltk.tokenize.WordPunctTokenizer().tokenize(x)\n",
    "    all_text.extend(x_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words are all the unique words in our entire training text set\n",
    "# We will use this to generate embedding layer later\n",
    "words = np.unique(all_text)\n",
    "n_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 81066\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size: {}\".format(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding at position 0, and 'zzzzzz' because for some reason, it was not in our unique words list\n",
    "word_index = ['_PADDING_'] +  ['zzzzzzz'] + list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['offeree',\n",
       " 'offerer',\n",
       " 'offering',\n",
       " 'offerings',\n",
       " 'offerman',\n",
       " 'offeror',\n",
       " 'offerred',\n",
       " 'offers',\n",
       " 'offguard',\n",
       " 'offhand']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests that our word_index works\n",
    "word_index[50595:50605]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='b'>b. Find maximum length of sentences to cap dataset at</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 183 µs, total: 252 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "# Find how long each sentence in our training set is, to see what max_sentence length is.\n",
    "# We need max_sent_len in order to do padding.\n",
    "# We need padding because we will MaxPool our words into sentences, so need consistent sentence length\n",
    "%%time\n",
    "sent_lengths = []\n",
    "for doc in df['token_ops']:\n",
    "    sents_in_doc = doc.split('##SENT##')\n",
    "    sent_lengths.append(len(sents_in_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1878., 2966., 2376., 1896., 1716., 1545.,  845.,  209.,   57.,\n",
       "          12.]),\n",
       " array([  2. ,  23.5,  45. ,  66.5,  88. , 109.5, 131. , 152.5, 174. ,\n",
       "        195.5, 217. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.sort(sent_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9168148148148149"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that 91.7% of our sentences have lengths less than 130 words\n",
    "# So we arbitrarily pick this as our max sentence length\n",
    "# NOTE: I tried longer sentence lengths, but the JupyterHub GPU crashes\n",
    "max_idx = np.argmax(np.sort(sent_lengths) >130)\n",
    "max_idx/13500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "# Set max words per sentence to 130\n",
    "# Use this size for maxpooling layer\n",
    "max_words_in_sent = 130 #max(sent_lengths)\n",
    "print(max_words_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for word index (vocabulary)\n",
    "word2idx = dict(zip(word_index, range(n_words+1)))\n",
    "idx2word = dict(zip(range(n_words+1), word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='b'>a. Generate X_train from cleaned data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.7 s, sys: 182 ms, total: 21.9 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert to numeric using word2idx and add padding\n",
    "X = []\n",
    "for doc in df_cleaned:\n",
    "    sents_in_doc = doc.split('newsenthere')\n",
    "    \n",
    "    mod_doc = []\n",
    "    for sent in sents_in_doc:\n",
    "        mod_sent=[]\n",
    "        x_tokens = nltk.tokenize.WordPunctTokenizer().tokenize(sent)\n",
    "        # Convert tokens in a sentence to index numbers\n",
    "        for token in x_tokens:\n",
    "            mod_sent.append(word2idx[token])\n",
    "        mod_doc.append(mod_sent[:max_words_in_sent])\n",
    "    X.append(pad_sequences(mod_doc, maxlen=max_words_in_sent, padding='post', value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the X_train\n",
    "X_train = []\n",
    "for doc in X:\n",
    "    concat_doc = []\n",
    "    for sent in doc:\n",
    "        concat_doc.extend(sent)\n",
    "    X_train.append(concat_doc)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length, by words: 27820\n"
     ]
    }
   ],
   "source": [
    "# find the max_doc_len\n",
    "doc_lengths = []\n",
    "for doc in X_train:\n",
    "    doc_lengths.append(len(doc))\n",
    "max_doc_len = max(doc_lengths)\n",
    "print(\"Max document length, by words: {}\".format(max_doc_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cap max senteces per doc to 110 sentences * 130 words per sent, cuz above max_length is too large\n",
    "sents_per_doc =110\n",
    "max_doc_len = sents_per_doc*max_words_in_sent\n",
    "max_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad documents so that they are all the same number of sentences\n",
    "X_tr_final = pad_sequences(X_train, maxlen=max_doc_len, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13500, 14300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>2. Pre-process Y_train labels</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to batching error, only these pre-processed files work\n",
    "# 2000 oracle iterations take 8 hours\n",
    "files=[\"oracle_batch4.txt\", \n",
    "      \"oracle_batch10.txt\", \n",
    "      \"oracle_batch12.txt\", \n",
    "    \"oracle_batch14.txt\",\n",
    "     \"oracle_batch15.txt\", \n",
    "    \"oracle_batch17.txt\",\n",
    "     \"oracle_batch18.txt\", \n",
    "    \"oracle_batch19.txt\"\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds unprocessed y_train (y_unproc) with a tuple of best sentence indices per document\n",
    "# Also builds F1 Rouge-2 scores, calculated during pre-processing\n",
    "# Borrowed from https://stackoverflow.com/questions/6633678/finding-words-after-keyword-in-python\n",
    "y_unproc=[]\n",
    "rouge_scores=[]\n",
    "\n",
    "for file in files:\n",
    "    f = open('./oracle/'+file, \"r\")\n",
    "    for line in f:\n",
    "        y_tup, split, rouge_score = line.partition('\\t')\n",
    "        rouge_score = rouge_score.strip('\\n')\n",
    "        y_unproc.append(y_tup)\n",
    "        rouge_scores.append(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices of N/A to drop later from x_train and y_train\n",
    "null_y = []\n",
    "for i in range(len(y_unproc)):\n",
    "    if y_unproc[i] == 'None':\n",
    "        null_y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of nulls: 70\n"
     ]
    }
   ],
   "source": [
    "print(\"# of nulls: {}\".format(len(null_y)))\n",
    "\n",
    "# Drop nulls\n",
    "X_tr_final = np.delete(X_tr_final, null_y, axis=0)\n",
    "y_unproc = np.delete(y_unproc, null_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13430,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13430"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set length of our y_train\n",
    "y_len = len(y_unproc)\n",
    "y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This generates the labels for each word in a sentence (binary 0 or 1, 1 being it is a chosen extracted summary sentence)\n",
    "\n",
    "# initialize zeros in the correct y_train shape\n",
    "y_full = np.zeros(shape=(y_len, sents_per_doc))\n",
    "for i in range(y_len):\n",
    "    if y_unproc[i] != 'None':\n",
    "        y_p = y_unproc[i].strip('(),').split(', ')\n",
    "        y_tpl = tuple(map(int, y_p))\n",
    "        for j in y_tpl:\n",
    "            if j < sents_per_doc:\n",
    "                y_full[i][j] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13430, 110)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'>3. Small Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3a'>a. Build embedding layer</a>\n",
    "Get GloVE word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/projects/glove/\n",
    "## I used wikipedia 2014+ Gigaword\n",
    "# Extract word vectors\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_doc_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Potentially use weighting to correct over imbalance between 0 and 1 y values\n",
    "\n",
    "#def weighted_bce(y_true, y_pred):\n",
    "#  weights = (y_true * 30.) + 1.\n",
    "#  bce = K.binary_crossentropy(y_true, y_pred)\n",
    "#  weighted_bce = K.mean(bce * weights)\n",
    "#  return weighted_bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3b'>b. Build RNN-RNN model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 14300)]           0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 14300, 50)         4053450   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 14300, 100)        30600     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 110, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 110, 100)          45600     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 110, 1)            101       \n",
      "=================================================================\n",
      "Total params: 4,129,751\n",
      "Trainable params: 76,301\n",
      "Non-trainable params: 4,053,450\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# architecture inspired by SummaRunner \n",
    "# https://github.com/hpzhao/SummaRuNNer/blob/master/models/RNN_RNN.py\n",
    "n_units=50\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0)\n",
    "#\"adam\"\n",
    "loss = 'binary_crossentropy'#weighted_bce\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "seq_input = Input(shape=(max_doc_len,))\n",
    "embedded_seq = embedding_layer(seq_input)\n",
    "\n",
    "# Word-level GRU\n",
    "x = Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True))(embedded_seq)\n",
    "\n",
    "# MaxPool combines words into sentences\n",
    "x = tf.keras.layers.MaxPool1D(pool_size = max_words_in_sent, padding='same')(x)\n",
    "\n",
    "# Sentence-level GRU after maxpooling all words in a sentence\n",
    "biGRU2 = Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True))(x)\n",
    "\n",
    "# Classification at the sentence level\n",
    "## Potential idea for next version: build more complex classifier layers or add attention layer\n",
    "output = TimeDistributed(Dense(units=1, activation='sigmoid'))(biGRU2)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=seq_input, outputs=output) \n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics,)# sample_weight_mode = 'temporal')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index at which to build validation set up to\n",
    "val_size = int(X_tr_final.shape[0]*0.1)\n",
    "\n",
    "# Shuffle indices randomly\n",
    "indices = np.arange(X_tr_final.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_tr_final = X_tr_final[indices]\n",
    "y_full = y_full[indices]\n",
    "\n",
    "# Generate Training and validation sets\n",
    "x_train = X_tr_final[val_size:]\n",
    "y_train = y_full[val_size:]\n",
    "x_val = X_tr_final[:val_size]\n",
    "y_val = y_full[:val_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12087 samples, validate on 1343 samples\n",
      "Epoch 1/5\n",
      "12087/12087 [==============================] - 540s 45ms/sample - loss: 0.1525 - accuracy: 0.9652 - val_loss: 0.1068 - val_accuracy: 0.9695\n",
      "Epoch 2/5\n",
      "12087/12087 [==============================] - 520s 43ms/sample - loss: 0.1045 - accuracy: 0.9690 - val_loss: 0.1029 - val_accuracy: 0.9694\n",
      "Epoch 3/5\n",
      "12087/12087 [==============================] - 519s 43ms/sample - loss: 0.1028 - accuracy: 0.9690 - val_loss: 0.1018 - val_accuracy: 0.9694\n",
      "Epoch 4/5\n",
      "12087/12087 [==============================] - 519s 43ms/sample - loss: 0.1020 - accuracy: 0.9690 - val_loss: 0.1011 - val_accuracy: 0.9695\n",
      "Epoch 5/5\n",
      "12087/12087 [==============================] - 520s 43ms/sample - loss: 0.1012 - accuracy: 0.9690 - val_loss: 0.1005 - val_accuracy: 0.9694\n",
      "CPU times: user 33min 22s, sys: 9min 29s, total: 42min 52s\n",
      "Wall time: 43min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train model\n",
    "verbose = 1\n",
    "\n",
    "#callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=4)\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5,#epochs, \n",
    "                    validation_data=(x_val, y_val), verbose=verbose,\n",
    "                    shuffle=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "#model.save_weights(\"{}.h5\".format('extract_model_v3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"extract_model_v3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'>4. Predict Summary and Score Summary</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate probability predictions for each sentence in each document\n",
    "y_val_prob = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the SummaRunner article, we decide on our best sentences, NOT by p>0.5,\n",
    "# because our positive true labels are too sparse. Instead, pick 5 largest probabilities\n",
    "# per document for our 5-sentence summaries.\n",
    "val_best_sents = []\n",
    "for i in range(len(y_val_prob)):\n",
    "    # Get first 5 sentence indices with largest probabilities\n",
    "    best = np.argsort(y_val_prob[i].reshape(-1))[::-1][0:5]\n",
    "    val_best_sents.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds document and loads it for scoring\n",
    "from Document import Document\n",
    "\n",
    "def load_data(src_file, tgt_file):\n",
    "    docs = []\n",
    "    for src_line, tgt_line in zip(src_file, tgt_file):\n",
    "        src_line = src_line.strip()\n",
    "        tgt_line = tgt_line.strip()\n",
    "        #if src_line == \"\" or tgt_line == \"\":\n",
    "        #    docs.append(None)\n",
    "        #    continue\n",
    "        src_sents = src_line.split('##SENT##')\n",
    "        tgt_sents = tgt_line.strip().split('##SENT##')\n",
    "        docs.append(Document(src_sents, tgt_sents))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13430, 5)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(null_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls from original imported DF and then select the validation indices\n",
    "val_df = df.drop(null_y).iloc[indices][:val_size]\n",
    "docs = load_data(val_df['token_ops'], val_df['token_heads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4b'>b. Find Rouge Scores</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRouge.Rouge.Rouge import Rouge\n",
    "\n",
    "rouge = Rouge(use_ngram_buf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-generated summary's Rouge-2 F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pred=[]\n",
    "for comb, document in zip(val_best_sents, docs):\n",
    "    c_string=[]\n",
    "    for idx in comb:\n",
    "        if idx < document.doc_len:\n",
    "            c_string.append(document.doc_sents[idx])\n",
    "    score = rouge.compute_rouge([document.summary_sents], [c_string])\n",
    "    scores_pred.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Rouge-2 F1 Score: 0.14429496859829427\n"
     ]
    }
   ],
   "source": [
    "rouge2_f_scores_pred =[]\n",
    "for score in scores_pred:\n",
    "    f_score = score['rouge-2']['f'][0]\n",
    "    rouge2_f_scores_pred.append(f_score)\n",
    "\n",
    "# Find average rouge score for all documents in validation set\n",
    "model_score = sum(rouge2_f_scores_pred)/len(rouge2_f_scores_pred)\n",
    "\n",
    "print(\"Model's Rouge-2 F1 Score: {}\".format(model_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Rouge-2 F1 Score based off of our \"true\" y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_true = pd.Series(rouge_scores).iloc[indices][:val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see if Rouge-2 score averages match what we outputted in preprocessing\n",
    "rouge_pre_proc = []\n",
    "for i in rouge_true:\n",
    "    rouge_pre_proc.append(np.float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label's Rouge-2 F1 Score: 0.25884311878219274\n"
     ]
    }
   ],
   "source": [
    "true_score = sum(rouge_pre_proc)/len(rouge_pre_proc)\n",
    "\n",
    "print(\"True label's Rouge-2 F1 Score: {}\".format(true_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to make code mor legible later\n",
    "def score_one(comb, document):\n",
    "    c_string=[]\n",
    "    for idx in comb:\n",
    "        if idx < document.doc_len:\n",
    "            c_string.append(document.doc_sents[idx])\n",
    "    score = rouge.compute_rouge([document.summary_sents], [c_string])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_one(y_true_combos[0], docs2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True average rouge score for our entire training set \n",
    "Based off true y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to drop null, or y's will not align\n",
    "docs2 = load_data(df['token_ops'].drop(null_y), df['token_heads'].drop(null_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13430"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that our document # and # of y labels match\n",
    "display(len(docs2))\n",
    "display(len(y_unproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_combos = []\n",
    "for doc in y_unproc:\n",
    "    if doc != 'None':\n",
    "        y_p = doc.strip('(),').split(', ')\n",
    "        y_tpl = tuple(map(int, y_p))\n",
    "        y_true_combos.append(y_tpl)\n",
    "    else:\n",
    "        y_true_combos.append((9999999,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_true=[]\n",
    "for comb, document in zip(y_true_combos, docs2):\n",
    "    c_string=[]\n",
    "    for idx in comb:\n",
    "        if idx < document.doc_len:\n",
    "            c_string.append(document.doc_sents[idx])\n",
    "    score = rouge.compute_rouge([document.summary_sents], [c_string])\n",
    "    scores_true.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average true score for entire dataset: 0.25657885279697557\n"
     ]
    }
   ],
   "source": [
    "rouge2_f_scores =[]\n",
    "for score in scores_true:\n",
    "    f_score = score['rouge-2']['f'][0]\n",
    "    rouge2_f_scores.append(f_score)\n",
    "    \n",
    "avg_true_score = sum(rouge2_f_scores)/len(rouge2_f_scores)\n",
    "print(\"The average true score for entire dataset: {}\".format(avg_true_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
