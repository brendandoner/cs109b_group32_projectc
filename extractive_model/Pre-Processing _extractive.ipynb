{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/20765011/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import lzma\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/20765011/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to remove \\n and HTML tags\n",
    "# function adapted from https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    text_divided = text.splitlines()\n",
    "    text_divided_clean = \" \".join(text_divided)\n",
    "    text_divided_clean = text_divided_clean.lower()\n",
    "    # DS: I re-added this line to remove parantheses\n",
    "    text_divided_clean = re.sub(r'\\([^)]*\\)', '', text_divided_clean)\n",
    "    text_divided_clean = re.sub('\"','', text_divided_clean) # remove '\"'\n",
    "    text_divided_clean = re.sub(r\"'s\\b\",\"\",text_divided_clean) # remove ''s'\n",
    "    \n",
    "    #NOTE: This below line removes all punctuation...so we are only\n",
    "    #training at the word-level, not sentence level\n",
    "    text_divided_clean = re.sub(\"[^a-zA-Z(##SENT##)]\", \" \", text_divided_clean) # removes all strings that contains a non-letter\n",
    "    tokens = [w for w in text_divided_clean.split() if not w in stop_words] \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word (number of letters < 3)\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary cleaner code MUST have punctuation and should NOT get rid of \n",
    "# short words like \"the\" or \"and\". \n",
    "# Code borrowed from: https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    \n",
    "    #added line to remove parantheses\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    #newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    \n",
    "    #Keep periods, remove other punctuation\n",
    "    # borrowed from: https://stackoverflow.com/questions/21209024/python-regex-remove-all-punctuation-except-hyphen-for-unicode-string\n",
    "    #remove = string.punctuation\n",
    "    #remove = remove.replace(\".\", \"\") # don't remove period or commas\n",
    "    remove = '][;()!\"#$%&*+-'\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    newString = re.sub(pattern,\"\",newString)\n",
    "    \n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "with lzma.open(\"./NC_text/data/data.jsonl.xz\", 'r') as jsonl_file:\n",
    "    for case in jsonl_file:\n",
    "        cases.append(json.loads(str(case, 'utf-8')))\n",
    "\n",
    "df = pd.DataFrame(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headnotes = []\n",
    "opinions = []\n",
    "\n",
    "for c in cases:\n",
    "    head = c['casebody']['data']['head_matter']\n",
    "    op = c['casebody']['data']['opinions']\n",
    "    \n",
    "    #Do not append empty headnotes or empty opinion text\n",
    "    if head and op:\n",
    "        headnotes.append(head)\n",
    "        opinions.append(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ops = []\n",
    "for op in opinions:\n",
    "    text_ops.append(op[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A majority of the court, therefore, being of the latter opinion, the bill of indictment was pronounced exceptionable: consequently, upon it sentence of death cannot bo passed , . 1 upon the prisoner.\\nThe keeper of the jail having received a mittimus to retain the prisoner, he will of course remain in jail until October term of the superior court, when a new bill will be drawn, and another trial will take place.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The keeper of the jail having received a mittimus to retain the prisoner, he will of course remain in jail until October term of the superior court, when a new bill will be drawn, and another trial will take place.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sentence tokenizer by nltk\n",
    "sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "text_ops[0]\n",
    "headnotes[0]\n",
    "\n",
    "results = sentence_tokenizer.tokenize(text_ops[0])\n",
    "display(text_ops[0])\n",
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_results = ' ##SENT## '.join(results)\n",
    "#mod_results = mod_results + \"test #4 ok\"\n",
    "\n",
    "# setting up tokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "full_tokens = tokenizer.tokenize(mod_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A majority of the court , therefore , being of the latter opinion , the bill of indictment was pronounced exceptionable : consequently , upon it sentence of death cannot bo passed , . ##SENT## 1 upon the prisoner . ##SENT## The keeper of the jail having received a mittimus to retain the prisoner , he will of course remain in jail until October term of the superior court , when a new bill will be drawn , and another trial will take place .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(full_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min, sys: 1.25 s, total: 5min 1s\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_body_word_count = 3000\n",
    "token_ops = []\n",
    "op_word_counts = []\n",
    "for i, op in enumerate(text_ops):\n",
    "    #if i % 5000 == 0:\n",
    "    #    display(i)\n",
    "    results = sentence_tokenizer.tokenize(op)\n",
    "    results = ' ##SENT## '.join(results)\n",
    "    # Cut off tokens at max allowed count\n",
    "    tokens = tokenizer.tokenize(results)[:max_body_word_count]\n",
    "    op_word_counts.append(len(tokens))\n",
    "    results = ' '.join(tokens)\n",
    "    token_ops.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 496 ms, total: 2min 12s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_head_word_count = 2000\n",
    "token_heads = []\n",
    "head_word_counts = []\n",
    "for i, head in enumerate(headnotes):\n",
    "    results = sentence_tokenizer.tokenize(head)\n",
    "    results = ' ##SENT## '.join(results)\n",
    "    tokens = tokenizer.tokenize(results)[:max_head_word_count]\n",
    "    head_word_counts.append(len(tokens))\n",
    "    results = ' '.join(tokens)\n",
    "    token_heads.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(token_ops, token_heads, op_word_counts, head_word_counts)), columns=('token_ops', 'token_heads', 'op counts', 'head counts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24603"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all cases with headnotes under 50 words\n",
    "df = df.loc[df['head counts']>50]\n",
    "\n",
    "#Caps on headnotes and summaries\n",
    "#df = df.loc[df['op counts']<3000]\n",
    "#df = df.loc[df['head counts']<2000]\n",
    "\n",
    "# Drop all cases where summaries are longer than 1/2 word-length of body text\n",
    "df = df.loc[df['op counts']>df['head counts']*2]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_START_ case no. 4,170. dunlop et al. v. west. brunner, col. cas. 27 hayw. n. c. 346. circuit court, d. north carolina. 1805. reported by albert brunner, esq., and here reprinted by permission.  _END_'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_START_ ' + summary_cleaner(headnotes[1]) + ' _END_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2'>2. Oracle</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with single doc\n",
    "doc = Document(token_ops[10000].split('##SENT##'),token_heads[10000].strip().split('##SENT##'))\n",
    "solve_one(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from: https://github.com/magic282/cnndm_acl18/blob/master/find_oracle.py\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "import gc\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from PyRouge.Rouge.Rouge import Rouge\n",
    "from Document import Document\n",
    "\n",
    "rouge = Rouge(use_ngram_buf=True)\n",
    "\n",
    "MAX_COMB_L = 5\n",
    "MAX_COMB_NUM = 100000\n",
    "\n",
    "\n",
    "def c_n_x(n, x):\n",
    "    if x > (n >> 2):\n",
    "        x = n - x\n",
    "    res = 1\n",
    "    for i in range(n, n - x, -1):\n",
    "        res *= i\n",
    "    for i in range(x, 0, -1):\n",
    "        res = res // i\n",
    "    return res\n",
    "\n",
    "\n",
    "def solve_one(document):\n",
    "    if document.doc_len == 0 or document.summary_len == 0:\n",
    "        return None, 0\n",
    "    sentence_bigram_recall = [0] * document.doc_len\n",
    "    for idx, sent in enumerate(document.doc_sents):\n",
    "        scores = rouge.compute_rouge([document.summary_sents], [sent])\n",
    "        recall = scores['rouge-2']['r'][0]\n",
    "        sentence_bigram_recall[idx] = recall\n",
    "    candidates = []\n",
    "    for idx, recall in enumerate(sentence_bigram_recall):\n",
    "        if recall > 0:\n",
    "            candidates.append(idx)\n",
    "    all_best_l = 1\n",
    "    all_best_score = 0\n",
    "    all_best_comb = None\n",
    "    for l in range(1, len(candidates)):\n",
    "        if l > MAX_COMB_L:\n",
    "            #print('Exceed MAX_COMB_L')\n",
    "            break\n",
    "        comb_num = c_n_x(len(candidates), l)\n",
    "        if math.isnan(comb_num) or math.isinf(comb_num) or comb_num > MAX_COMB_NUM:\n",
    "            #print('Exceed MAX_COMB_NUM')\n",
    "            break\n",
    "        combs = itertools.combinations(candidates, l)\n",
    "        l_best_score = 0\n",
    "        l_best_choice = None\n",
    "        for comb in combs:\n",
    "            c_string = [document.doc_sents[idx] for idx in comb]\n",
    "            rouge_scores = rouge.compute_rouge([document.summary_sents], [c_string])\n",
    "            rouge_bigram_f1 = rouge_scores['rouge-2']['f'][0]\n",
    "            if rouge_bigram_f1 > l_best_score:\n",
    "                l_best_score = rouge_bigram_f1\n",
    "                l_best_choice = comb\n",
    "        if l_best_score > all_best_score:\n",
    "            all_best_l = l\n",
    "            all_best_score = l_best_score\n",
    "            all_best_comb = l_best_choice\n",
    "        else:\n",
    "            if l > all_best_l:\n",
    "                break\n",
    "    return all_best_comb, all_best_score\n",
    "\n",
    "\n",
    "def solve(documents, output_file):\n",
    "    writer = open(output_file, 'w', encoding='utf-8', buffering=1)\n",
    "    for idx, doc in enumerate(documents):\n",
    "        if idx % 100 == 0:\n",
    "            print(datetime.datetime.now())\n",
    "            rouge.ngram_buf = {}\n",
    "            gc.collect()\n",
    "        comb = solve_one(doc)\n",
    "        writer.write('{0}\\t {1}'.format(comb[0], comb[1]) + '\\n')\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def load_data(src_file, tgt_file):\n",
    "    docs = []\n",
    "    for src_line, tgt_line in zip(src_file, tgt_file):\n",
    "        src_line = src_line.strip()\n",
    "        tgt_line = tgt_line.strip()\n",
    "        #if src_line == \"\" or tgt_line == \"\":\n",
    "        #    docs.append(None)\n",
    "        #    continue\n",
    "        src_sents = src_line.split('##SENT##')\n",
    "        tgt_sents = tgt_line.strip().split('##SENT##')\n",
    "        docs.append(Document(src_sents, tgt_sents))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def find_highlights(src_file, tgt_file, outfile_name):\n",
    "    docs = load_data(src_file, tgt_file)\n",
    "    solve(docs, outfile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-02 01:40:29.932713\n",
      "2020-05-02 02:00:52.841746\n",
      "2020-05-02 02:19:23.985896\n",
      "2020-05-02 02:43:47.179619\n",
      "2020-05-02 03:07:46.672440\n",
      "2020-05-02 03:32:03.611756\n",
      "2020-05-02 03:57:14.912930\n",
      "2020-05-02 04:13:57.926485\n",
      "2020-05-02 04:33:55.306883\n",
      "2020-05-02 04:56:51.450236\n",
      "CPU times: user 3h 42min 36s, sys: 4.54 s, total: 3h 42min 41s\n",
      "Wall time: 3h 42min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = 5000\n",
    "end = 6000\n",
    "find_highlights(df['token_ops'][start:end], df['token_heads'][start:end], \"oracle_batch9.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
