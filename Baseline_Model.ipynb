{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qilongxin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import json\n",
    "import lzma\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# pd.options.display.max_columns = 999\n",
    "# pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fucnction to remove \\n and HTML tags\n",
    "# function adapted from https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    text_divided = text.splitlines()\n",
    "    text_divided_clean = \" \".join(text_divided)\n",
    "    text_divided_clean = text_divided_clean.lower()\n",
    "    text_divided_clean = re.sub('\"','', text_divided_clean) # remove '\"'\n",
    "    text_divided_clean = re.sub(r\"'s\\b\",\"\",text_divided_clean) # remove ''s'\n",
    "    text_divided_clean = re.sub(\"[^a-zA-Z]\", \" \", text_divided_clean) # removes all strings that contains a non-letter\n",
    "    return text_divided_clean\n",
    "\n",
    "\n",
    "# setting up tokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(state):\n",
    "    # reading json files\n",
    "    cases = []\n",
    "    with lzma.open(state + '/data/data.jsonl.xz', 'r') as jsonl_file:\n",
    "        for case in jsonl_file:\n",
    "            cases.append(json.loads(str(case, 'utf-8')))\n",
    "\n",
    "    df = pd.DataFrame(cases).sort_values('decision_date').reset_index(drop=True)\n",
    "    df['decision_date'] = pd.to_datetime(df['decision_date'])\n",
    "\n",
    "    # parsing data\n",
    "    storage = []\n",
    "    for i in range(df.shape[0]):\n",
    "        casebody_idx = df.columns.get_loc(\"casebody\")\n",
    "        judges = df.iloc[i,casebody_idx]['data']['judges']\n",
    "        attorneys = df.iloc[i,casebody_idx]['data']['attorneys']\n",
    "        headnotes = df.iloc[i,casebody_idx]['data']['head_matter']\n",
    "        if df.iloc[i,casebody_idx]['data']['opinions'] != []:\n",
    "            opinions = df.iloc[i,casebody_idx]['data']['opinions'][0]['text']\n",
    "\n",
    "#         headnotes_clean = text_cleaner(headnotes)\n",
    "#         opinions_clean = text_cleaner(opinions)\n",
    "\n",
    "        storage.append({'judges': judges,\n",
    "                        'attorneys': attorneys,\n",
    "                        'headnotes': headnotes,\n",
    "                        'opinions': opinions})\n",
    "    df_parsed = pd.DataFrame(storage)\n",
    "    df = df_parsed.merge(df, left_index=True, right_index=True)\n",
    "\n",
    "    # tokenizing headnotes and opinions\n",
    "    df['headnotes_token'] = df['headnotes'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    df['opinions_token'] = df['opinions'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    df['headnotes_num_tokens'] = [len(notes) for notes in df['headnotes_token']]\n",
    "    df['opinions_num_tokens'] = [len(opinions) for opinions in df['opinions_token']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = get_data('Arkansas')\n",
    "\n",
    "above_mean_summary = df_ar['headnotes_num_tokens'] > np.mean(df_ar.headnotes_num_tokens)\n",
    "above_mean_text = df_ar['opinions_num_tokens'] > np.mean(df_ar.opinions_num_tokens)\n",
    "\n",
    "df_ar_to_train = df_ar[above_mean_summary & above_mean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "\n",
    "train = df_ar_to_train.sample(n=train_size,replace=False, random_state=1)\n",
    "x_train,y_train = train.opinions.tolist(),train.headnotes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3318770278665225\n"
     ]
    }
   ],
   "source": [
    "# df_ar_to_train.to_pickle('df_ar_to_train.pkl')\n",
    "from summarizer import Summarizer\n",
    "#https://github.com/dmmiller612/bert-extractive-summarizer\n",
    "import rouge \n",
    "\n",
    "model = Summarizer()\n",
    "rouge = rouge.Rouge()\n",
    "\n",
    "scores = np.zeros(len(x_train))\n",
    "for i,text in enumerate(x_train): \n",
    "    result = model(text, min_length=60)\n",
    "    full = ''.join(result)\n",
    "    sc = rouge.get_scores(y_train[i], full)\n",
    "    scores[i] = sc[0]['rouge-1']['f'] \n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
