{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import lzma\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing for Extractive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1'>1. Tokenizing sentences and words</a>\n",
    "Unlike abstractive modeling, we need to tokenize by sentence as well as by words for this particular extractive model. Our model needs data that has clear sentence markers, to be able to extract/select the best sentences for making a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "#Rename path for North Carolina cases, .txt files, on your own machine\n",
    "with lzma.open(\"./NC_text/data/data.jsonl.xz\", 'r') as jsonl_file:\n",
    "    for case in jsonl_file:\n",
    "        cases.append(json.loads(str(case, 'utf-8')))\n",
    "\n",
    "headnotes = []\n",
    "text_ops = []\n",
    "\n",
    "for c in cases:\n",
    "    head = c['casebody']['data']['head_matter']\n",
    "    op = c['casebody']['data']['opinions']\n",
    "    \n",
    "    #Do not append empty headnotes or empty opinion text\n",
    "    if head and op:\n",
    "        headnotes.append(head)\n",
    "        text_ops.append(op[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cap body texts to 3000 words, due to limits of our GPU\n",
    "max_body_word_count = 3000\n",
    "token_ops = []\n",
    "op_word_counts = []\n",
    "for i, op in enumerate(text_ops):\n",
    "    results = sentence_tokenizer.tokenize(op)\n",
    "    results = ' ##SENT## '.join(results)\n",
    "    # Cut off tokens at max allowed count\n",
    "    tokens = tokenizer.tokenize(results)[:max_body_word_count]\n",
    "    op_word_counts.append(len(tokens))\n",
    "    results = ' '.join(tokens)\n",
    "    token_ops.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cap head notes to 2000 words\n",
    "max_head_word_count = 2000\n",
    "token_heads = []\n",
    "head_word_counts = []\n",
    "for i, head in enumerate(headnotes):\n",
    "    results = sentence_tokenizer.tokenize(head)\n",
    "    results = ' ##SENT## '.join(results)\n",
    "    tokens = tokenizer.tokenize(results)[:max_head_word_count]\n",
    "    head_word_counts.append(len(tokens))\n",
    "    results = ' '.join(tokens)\n",
    "    token_heads.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all cases with headnotes under 50 words, because our EDA shows\n",
    "# that they tend to not be true head notes, but just list of names\n",
    "df = pd.DataFrame(list(zip(token_ops, token_heads, op_word_counts, head_word_counts)), columns=('token_ops', 'token_heads', 'op counts', 'head counts'))\n",
    "df = df.loc[df['head counts']>50]\n",
    "\n",
    "# Drop all cases where summaries are longer than 1/2 word-length of body text\n",
    "# Mainly because when headnotes are as long as their opinion text,\n",
    "# they arguably are not really \"summaries\" anymore by that point.\n",
    "df = df.loc[df['op counts']>df['head counts']*2]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv to prepare for Oracle step\n",
    "df[:24000].to_csv(\"true_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2'>2. Oracle</a>\n",
    "This is used as an unsupervised way to generate the \"true labels\" (e.g. best sentences to extract) from the true head note. \n",
    "This particular algorithm is from the Zhou et al piece \"Neural Document Summmarization by Jointly Learning to Score and Select Sentences,\" found here: https://www.aclweb.org/anthology/P18-1061.pdf. The model they use is called NeuSum.\n",
    "\n",
    "This algorithm constructs the training data set by labelling sentences in a given document as part of the summary or not, based on maximizing the Rouge-2 F1 score. \n",
    "\n",
    "Because our documents for these cases are so much longer than the CNN/Daily Mail dataset used by Zhou et al for NeuSum, the below process takes far longer to find labels, and also it does not explore all possible combinations of sentences--otherwise, the algorithm would take far too long to find all the best labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from: https://github.com/magic282/cnndm_acl18/blob/master/find_oracle.py\n",
    "\n",
    "# Calculates Rouge Scores\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import scipy.stats as st\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "class Rouge(object):\n",
    "    def __init__(self, stem=True, use_ngram_buf=False):\n",
    "        self.N = 2\n",
    "        self.stem = stem\n",
    "        self.use_ngram_buf = use_ngram_buf\n",
    "        self.ngram_buf = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_sentence(sentence):\n",
    "        s = sentence.lower()\n",
    "        s = re.sub(r\"[^0-9a-z]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        s = s.strip()\n",
    "        return s\n",
    "\n",
    "    def _create_n_gram(self, raw_sentence, n, stem):\n",
    "        if self.use_ngram_buf:\n",
    "            if raw_sentence in self.ngram_buf:\n",
    "                return self.ngram_buf[raw_sentence]\n",
    "        res = {}\n",
    "        sentence = Rouge._format_sentence(raw_sentence)\n",
    "        tokens = sentence.split(' ')\n",
    "        if stem:\n",
    "            # try:  # TODO older NLTK has a bug in Porter Stemmer\n",
    "            tokens = [stemmer.stem(t) for t in tokens]\n",
    "            # except:\n",
    "            #     pass\n",
    "        sent_len = len(tokens)\n",
    "        for _n in range(n):\n",
    "            buf = Counter()\n",
    "            for idx, token in enumerate(tokens):\n",
    "                if idx + _n >= sent_len:\n",
    "                    break\n",
    "                ngram = ' '.join(tokens[idx: idx + _n + 1])\n",
    "                buf[ngram] += 1\n",
    "            res[_n] = buf\n",
    "        if self.use_ngram_buf:\n",
    "            self.ngram_buf[raw_sentence] = res\n",
    "        return res\n",
    "\n",
    "    def get_ngram(self, sents, N, stem=False):\n",
    "        if isinstance(sents, list):\n",
    "            res = {}\n",
    "            for _n in range(N):\n",
    "                res[_n] = Counter()\n",
    "            for sent in sents:\n",
    "                ngrams = self._create_n_gram(sent, N, stem)\n",
    "                for this_n, counter in ngrams.items():\n",
    "                    # res[this_n] = res[this_n] + counter\n",
    "                    self_counter = res[this_n]\n",
    "                    for elem, count in counter.items():\n",
    "                        if elem not in self_counter:\n",
    "                            self_counter[elem] = count\n",
    "                        else:\n",
    "                            self_counter[elem] += count\n",
    "            return res\n",
    "        elif isinstance(sents, str):\n",
    "            return self._create_n_gram(sents, N, stem)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def get_mean_sd_internal(self, x):\n",
    "        mean = np.mean(x)\n",
    "        sd = st.sem(x)\n",
    "        res = st.t.interval(0.95, len(x) - 1, loc=mean, scale=sd)\n",
    "        return (mean, sd, res)\n",
    "\n",
    "    def compute_rouge(self, references, systems):\n",
    "        assert (len(references) == len(systems))\n",
    "\n",
    "        peer_count = len(references)\n",
    "\n",
    "        result_buf = {}\n",
    "        for n in range(self.N):\n",
    "            result_buf[n] = {'p': [], 'r': [], 'f': []}\n",
    "\n",
    "        for ref_sent, sys_sent in zip(references, systems):\n",
    "            ref_ngrams = self.get_ngram(ref_sent, self.N, self.stem)\n",
    "            sys_ngrams = self.get_ngram(sys_sent, self.N, self.stem)\n",
    "            for n in range(self.N):\n",
    "                ref_ngram = ref_ngrams[n]\n",
    "                sys_ngram = sys_ngrams[n]\n",
    "                ref_count = sum(ref_ngram.values())\n",
    "                sys_count = sum(sys_ngram.values())\n",
    "                match_count = 0\n",
    "                for k, v in sys_ngram.items():\n",
    "                    if k in ref_ngram:\n",
    "                        match_count += min(v, ref_ngram[k])\n",
    "                p = match_count / sys_count if sys_count != 0 else 0\n",
    "                r = match_count / ref_count if ref_count != 0 else 0\n",
    "                f = 0 if (p == 0 or r == 0) else 2 * p * r / (p + r)\n",
    "                result_buf[n]['p'].append(p)\n",
    "                result_buf[n]['r'].append(r)\n",
    "                result_buf[n]['f'].append(f)\n",
    "\n",
    "        res = {}\n",
    "        for n in range(self.N):\n",
    "            n_key = 'rouge-{0}'.format(n + 1)\n",
    "            res[n_key] = {}\n",
    "            if len(result_buf[n]['p']) >= 50:\n",
    "                res[n_key]['p'] = self.get_mean_sd_internal(result_buf[n]['p'])\n",
    "                res[n_key]['r'] = self.get_mean_sd_internal(result_buf[n]['r'])\n",
    "                res[n_key]['f'] = self.get_mean_sd_internal(result_buf[n]['f'])\n",
    "            else:\n",
    "                # not enough samples to calculate confidence interval\n",
    "                res[n_key]['p'] = (np.mean(np.array(result_buf[n]['p'])), 0, (0, 0))\n",
    "                res[n_key]['r'] = (np.mean(np.array(result_buf[n]['r'])), 0, (0, 0))\n",
    "                res[n_key]['f'] = (np.mean(np.array(result_buf[n]['f'])), 0, (0, 0))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from: https://github.com/magic282/cnndm_acl18/blob/master/find_oracle.py\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "import gc\n",
    "import math\n",
    "import datetime\n",
    "#from PyRouge.Rouge.Rouge import Rouge\n",
    "\n",
    "class Document(object):\n",
    "    def __init__(self, doc_sents, summary_sents):\n",
    "        self.doc_sents = doc_sents\n",
    "        self.summary_sents = summary_sents\n",
    "        self.doc_len = len(self.doc_sents)\n",
    "        self.summary_len = len(self.summary_sents)\n",
    "        self.concat_summary = \" \".join(self.summary_sents)\n",
    "\n",
    "rouge = Rouge(use_ngram_buf=True)\n",
    "\n",
    "MAX_COMB_L = 5\n",
    "MAX_COMB_NUM = 100000\n",
    "\n",
    "\n",
    "def c_n_x(n, x):\n",
    "    if x > (n >> 2):\n",
    "        x = n - x\n",
    "    res = 1\n",
    "    for i in range(n, n - x, -1):\n",
    "        res *= i\n",
    "    for i in range(x, 0, -1):\n",
    "        res = res // i\n",
    "    return res\n",
    "\n",
    "\n",
    "def solve_one(document):\n",
    "    if document.doc_len == 0 or document.summary_len == 0:\n",
    "        return None, 0\n",
    "    sentence_bigram_recall = [0] * document.doc_len\n",
    "    for idx, sent in enumerate(document.doc_sents):\n",
    "        scores = rouge.compute_rouge([document.summary_sents], [sent])\n",
    "        recall = scores['rouge-2']['r'][0]\n",
    "        sentence_bigram_recall[idx] = recall\n",
    "    candidates = []\n",
    "    for idx, recall in enumerate(sentence_bigram_recall):\n",
    "        if recall > 0:\n",
    "            candidates.append(idx)\n",
    "    all_best_l = 1\n",
    "    all_best_score = 0\n",
    "    all_best_comb = None\n",
    "    for l in range(1, len(candidates)):\n",
    "        if l > MAX_COMB_L:\n",
    "            #print('Exceed MAX_COMB_L')\n",
    "            break\n",
    "        comb_num = c_n_x(len(candidates), l)\n",
    "        if math.isnan(comb_num) or math.isinf(comb_num) or comb_num > MAX_COMB_NUM:\n",
    "            #print('Exceed MAX_COMB_NUM')\n",
    "            break\n",
    "        combs = itertools.combinations(candidates, l)\n",
    "        l_best_score = 0\n",
    "        l_best_choice = None\n",
    "        for comb in combs:\n",
    "            c_string = [document.doc_sents[idx] for idx in comb]\n",
    "            rouge_scores = rouge.compute_rouge([document.summary_sents], [c_string])\n",
    "            rouge_bigram_f1 = rouge_scores['rouge-2']['f'][0]\n",
    "            if rouge_bigram_f1 > l_best_score:\n",
    "                l_best_score = rouge_bigram_f1\n",
    "                l_best_choice = comb\n",
    "        if l_best_score > all_best_score:\n",
    "            all_best_l = l\n",
    "            all_best_score = l_best_score\n",
    "            all_best_comb = l_best_choice\n",
    "        else:\n",
    "            if l > all_best_l:\n",
    "                break\n",
    "    return all_best_comb, all_best_score\n",
    "\n",
    "\n",
    "def solve(documents, output_file):\n",
    "    writer = open(output_file, 'w', encoding='utf-8', buffering=1)\n",
    "    for idx, doc in enumerate(documents):\n",
    "        if idx % 100 == 0:\n",
    "            print(datetime.datetime.now())\n",
    "            rouge.ngram_buf = {}\n",
    "            gc.collect()\n",
    "        comb = solve_one(doc)\n",
    "        writer.write('{0}\\t {1}'.format(comb[0], comb[1]) + '\\n')\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def load_data(src_file, tgt_file):\n",
    "    docs = []\n",
    "    for src_line, tgt_line in zip(src_file, tgt_file):\n",
    "        src_line = src_line.strip()\n",
    "        tgt_line = tgt_line.strip()\n",
    "        src_sents = src_line.split('##SENT##')\n",
    "        tgt_sents = tgt_line.strip().split('##SENT##')\n",
    "        docs.append(Document(src_sents, tgt_sents))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def find_highlights(src_file, tgt_file, outfile_name):\n",
    "    docs = load_data(src_file, tgt_file)\n",
    "    solve(docs, outfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes 72+ hours of processing\n",
    "# This was run parallel on multiple machines to speed up processing\n",
    "start = 0#6000\n",
    "end = 2#24000\n",
    "find_highlights(df['token_ops'][start:end], df['token_heads'][start:end], \"full_oracle.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Extractive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1'>1. Further Pre-processing: X_train</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_df.csv can be found in this zip file that Song generated for paralelling pre-processing:\n",
    "# https://drive.google.com/file/d/1yDXjd7seRCp3_YZDHky3utDVtBdIsSYL/view?usp=sharing\n",
    "\n",
    "df = pd.read_csv(\"true_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust df to match the y_train that we pre-processed by batching\n",
    "df = df.iloc[:24000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='a'>a. Generate vocabulary via tokenization for embeddings</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't need to get embeddings for headnote tokens, because our\n",
    "# RNN will only extract sentences from the opinion body text;\n",
    "# thus, my model never needs to \"read\" the headnotes\n",
    "all_text = df['token_ops']\n",
    "\n",
    "# Text cleaner removes numbers and punctuation, as they are probably not needed\n",
    "# for the model to evaluate the IMPORTANCE of a sentence (not meaning).\n",
    "# It also decreases the size of our input, for the sake of our GPU\n",
    "\n",
    "def txt_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    \n",
    "    #added line to remove parantheses\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString) # remove '\"'\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    \n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# all_text: all the text, tokenized for generating the vocabulary, which we need for embedding\n",
    "# df_cleaned: the cleaned dataframe with punctuations/numbers removed\n",
    "\n",
    "all_text = []\n",
    "df_cleaned = []\n",
    "\n",
    "for doc in df['token_ops']:\n",
    "    x = doc.replace(\"##SENT##\", \"newsenthere\")\n",
    "    x = txt_cleaner(x)\n",
    "    df_cleaned.append(x)\n",
    "    \n",
    "    x_tok = nltk.tokenize.WordPunctTokenizer().tokenize(x)\n",
    "    all_text.extend(x_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words: all the unique words in our entire training text set\n",
    "# We will use this to generate embedding layer later\n",
    "words = np.unique(all_text)\n",
    "n_words = len(words)\n",
    "\n",
    "# Add padding at position 0, and 'zzzzzz' because for some reason, it was not in our unique words list\n",
    "word_index = ['_PADDING_'] +  ['zzzzzzz'] + list(words)\n",
    "\n",
    "print(\"Vocab size: {}\".format(n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='b'>b. Find maximum length of sentences to cap dataset at</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Find how long each sentence in our training set is, to see what max_sentence length is.\n",
    "# We need max_sent_len in order to do padding.\n",
    "# We need padding because we will MaxPool our words into sentences, so need consistent sentence length\n",
    "\n",
    "sent_lengths = []\n",
    "for doc in df['token_ops']:\n",
    "    sents_in_doc = doc.split('##SENT##')\n",
    "    sent_lengths.append(len(sents_in_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sort(sent_lengths), bins=50)\n",
    "plt.title(\"Length of sentences in dataset\")\n",
    "plt.xlabel(\"Word length\")\n",
    "plt.ylabel(\"# of sentences with x word length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that 91% of our sentences have lengths less than 130 words\n",
    "# So we arbitrarily pick this as our max sentence length\n",
    "# NOTE: I tried longer sentence lengths, but the JupyterHub GPU crashes\n",
    "max_idx = np.argmax(np.sort(sent_lengths) >130)\n",
    "max_idx/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max words per sentence to 130\n",
    "# Use this size for maxpooling layer\n",
    "max_words_in_sent = 130 #max(sent_lengths)\n",
    "print(max_words_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for word index (vocabulary)\n",
    "word2idx = dict(zip(word_index, range(n_words+1)))\n",
    "idx2word = dict(zip(range(n_words+1), word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='c'>c. Generate X_train from cleaned data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert to numeric using word2idx and add padding\n",
    "X = []\n",
    "for doc in df_cleaned:\n",
    "    sents_in_doc = doc.split('newsenthere')\n",
    "    \n",
    "    mod_doc = []\n",
    "    for sent in sents_in_doc:\n",
    "        mod_sent=[]\n",
    "        x_tokens = nltk.tokenize.WordPunctTokenizer().tokenize(sent)\n",
    "        # Convert tokens in a sentence to index numbers\n",
    "        for token in x_tokens:\n",
    "            mod_sent.append(word2idx[token])\n",
    "        mod_doc.append(mod_sent[:max_words_in_sent])\n",
    "    X.append(pad_sequences(mod_doc, maxlen=max_words_in_sent, padding='post', value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X_train\n",
    "X_train = []\n",
    "for doc in X:\n",
    "    concat_doc = []\n",
    "    for sent in doc:\n",
    "        concat_doc.extend(sent)\n",
    "    X_train.append(concat_doc)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap max senteces per doc to 110 sentences * 130 words per sent, cuz above max_length is too large\n",
    "sents_per_doc =110\n",
    "max_doc_len = sents_per_doc*max_words_in_sent\n",
    "max_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad documents so that they are all the same number of sentences\n",
    "X_tr_final = pad_sequences(X_train, maxlen=max_doc_len, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2'>2. Further Pre-processing: Y_train labels</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000 oracle iterations take 8 hours, so doing the full pre-processing\n",
    "# was not a feasible option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds unprocessed y_train (y_unproc) with a tuple of best sentence indices per document\n",
    "# Also builds F1 Rouge-2 scores, calculated during pre-processing\n",
    "\n",
    "# Borrowed from https://stackoverflow.com/questions/6633678/finding-words-after-keyword-in-python\n",
    "y_unproc=[]\n",
    "rouge_scores=[]\n",
    "\n",
    "f = open(\"./oracle/full_oracle.txt\", \"r\")\n",
    "for line in f:\n",
    "    y_tup, split, rouge_score = line.partition('\\t')\n",
    "    rouge_score = rouge_score.strip('\\n')\n",
    "    y_unproc.append(y_tup)\n",
    "    rouge_scores.append(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of N/A to drop later from x_train and y_train\n",
    "null_y = []\n",
    "for i in range(len(y_unproc)):\n",
    "    if y_unproc[i] == 'None':\n",
    "        null_y.append(i)\n",
    "\n",
    "print(\"# of nulls: {}\".format(len(null_y)))\n",
    "\n",
    "# Drop nulls\n",
    "X_tr_final = np.delete(X_tr_final, null_y, axis=0)\n",
    "y_unproc = np.delete(y_unproc, null_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set length of our y_train\n",
    "y_len = len(y_unproc)\n",
    "y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates the lbinary abels for each word in a sentence (0 or 1, 1 being it is a chosen extracted summary sentence)\n",
    "\n",
    "# initialize zeros in the correct y_train shape\n",
    "y_full = np.zeros(shape=(y_len, sents_per_doc))\n",
    "for i in range(y_len):\n",
    "    if y_unproc[i] != 'None':\n",
    "        y_p = y_unproc[i].strip('(),').split(', ')\n",
    "        y_tpl = tuple(map(int, y_p))\n",
    "        for j in y_tpl:\n",
    "            if j < sents_per_doc:\n",
    "                y_full[i][j] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3'>3. The Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3a'>a. Build embedding layer</a>\n",
    "Get GloVE word embeddings for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# I used wikipedia 2014+ Gigaword\n",
    "\n",
    "# Extract word vectors\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding matrix\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_doc_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3b'>b. Build GRU-GRU model</a>\n",
    "Our model draws predominantly from the Nallaptai, Zhai, and Zhou 2016 SummaRuNNer paper: https://arxiv.org/pdf/1611.04230.pdf.\n",
    "\n",
    "I have two bidirectional GRUs: One operates at the word level, and the other operates at the sentence level. Because we padded all our sentences to be the same number of words, we can use MaxPooling to consolidate all the word outputs into a single sentence output, and feed those into our 2nd GRU layer that looks at the sentence level.\n",
    "\n",
    "Due to the difficulty of building a classification layer that handles salience, novelty, absolute position, and relative position (it would have been easier to do in straight tensorflow or Pytorch), I opt for a simple Dense layer at the end to classify the labels (unlike the real SummaRuNNer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture inspired by SummaRunner \n",
    "# https://github.com/hpzhao/SummaRuNNer/blob/master/models/RNN_RNN.py\n",
    "n_units=50\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001,clipvalue=1.0)\n",
    "loss = 'binary_crossentropy'#weighted_bce\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "seq_input = Input(shape=(max_doc_len,))\n",
    "embedded_seq = embedding_layer(seq_input)\n",
    "\n",
    "# Word-level GRU\n",
    "x = Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True))(embedded_seq)\n",
    "\n",
    "# MaxPool combines words into sentences\n",
    "wordout = tf.keras.layers.MaxPool1D(pool_size = max_words_in_sent, padding='same')(x)\n",
    "\n",
    "# Sentence-level GRU after maxpooling all words in a sentence\n",
    "sentout = Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True))(wordout)\n",
    "\n",
    "# Classification at the sentence level\n",
    "output = TimeDistributed(Dense(units=1, activation='sigmoid'))(sentout)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=seq_input, outputs=output) \n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics,)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index at which to build validation set up\n",
    "val_size = int(X_tr_final.shape[0]*0.1)\n",
    "\n",
    "# Shuffle indices randomly\n",
    "np.random.seed(2143)\n",
    "indices = np.arange(X_tr_final.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_tr_final = X_tr_final[indices]\n",
    "y_full = y_full[indices]\n",
    "\n",
    "# Generate Training and validation sets\n",
    "x_train = X_tr_final[val_size:]\n",
    "y_train = y_full[val_size:]\n",
    "x_val = X_tr_final[:val_size]\n",
    "y_val = y_full[:val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model\n",
    "verbose = 1\n",
    "\n",
    "callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=1)\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5,#epochs, \n",
    "                    validation_data=(x_val, y_val), verbose=verbose,\n",
    "                    shuffle=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "model.save_weights(\"{}.h5\".format('extract_model_v3_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "model.load_weights(\"extract_model_v3_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4'>4. Generating and Scoring Summary</a>\n",
    "We use both the Rouge-1 and Rouge-2 F1 scores in order to measure how well our predicted/generated summaries match up to the true summaries.\n",
    "The Rouge-2 F1 score is used in both the NeuSum and SummaRunner papers that influence this extractive model, primarily because matching bigrams is a more complex task than matching unigrams (Rouge-1), and the F1 score includes both precision and recall, making it a more \"well-rounded\" measure of how well our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate probability predictions for each sentence in each document\n",
    "y_val_prob = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the SummaRunner article, we decide on our best sentences, NOT by p>0.5,\n",
    "# because our positive true labels are too sparse. Instead, pick 5 largest probabilities\n",
    "# per document for our 5-sentence summaries.\n",
    "val_best_sents = []\n",
    "for i in range(len(y_val_prob)):\n",
    "    # Get first 5 sentence indices with largest probabilities\n",
    "    best = np.argsort(y_val_prob[i].reshape(-1))[::-1][0:5]\n",
    "    val_best_sents.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds document and loads it for scoring\n",
    "# Uses custom object, Document, based off Summarunner code\n",
    "\n",
    "def load_data(src_file, tgt_file):\n",
    "    docs = []\n",
    "    for src_line, tgt_line in zip(src_file, tgt_file):\n",
    "        src_line = src_line.strip()\n",
    "        tgt_line = tgt_line.strip()\n",
    "        #if src_line == \"\" or tgt_line == \"\":\n",
    "        #    docs.append(None)\n",
    "        #    continue\n",
    "        src_sents = src_line.split('##SENT##')\n",
    "        tgt_sents = tgt_line.strip().split('##SENT##')\n",
    "        docs.append(Document(src_sents, tgt_sents))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls from original imported DF and then select the validation indices\n",
    "val_df = df.drop(null_y).iloc[indices][:val_size]\n",
    "docs = load_data(val_df['token_ops'], val_df['token_heads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4a'>a. Find Rouge Score</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge(use_ngram_buf=True)\n",
    "\n",
    "# Generate scores using Py Rouge\n",
    "def score_one(comb, document):\n",
    "    c_string=[]\n",
    "    for idx in comb:\n",
    "        if idx < document.doc_len:\n",
    "            c_string.append(document.doc_sents[idx])\n",
    "    score = rouge.compute_rouge([document.summary_sents], [c_string])\n",
    "    return score\n",
    "\n",
    "def get_scores(best_sents, documents):\n",
    "    scores_pred=[]\n",
    "    for comb, document in zip(best_sents, documents):\n",
    "        score = score_one(comb, document)\n",
    "        scores_pred.append(score)\n",
    "    return scores_pred\n",
    "\n",
    "# Use scores_pred, the output from get_scores, to get average score\n",
    "# rouge_type is string \"Rouge-1\" or \"Rouge-2\"\n",
    "# p_r_f: select 'p', 'r', or 'f'\n",
    "def find_avg_rouge(scores_pred, rouge_type, p_r_f):\n",
    "    rouge_scores_pred =[]\n",
    "    for score in scores_pred:\n",
    "        f_score = score[rouge_type][p_r_f][0]\n",
    "        rouge_scores_pred.append(f_score)\n",
    "    \n",
    "    avg_score = sum(rouge_scores_pred)/len(rouge_scores_pred)\n",
    "    \n",
    "    return avg_score #{'avg score': avg_score, 'all scores':rouge_scores_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-generated summary's Rouge-2 F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_scores = get_scores(val_best_sents, docs)\n",
    "model_r1_score = find_avg_rouge(model_scores, \"rouge-1\", \"f\")\n",
    "model_r2_score = find_avg_rouge(model_scores, \"rouge-2\", \"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's Rouge-1 F1 Score: {}\".format(model_r1_score))\n",
    "print(\"Model's Rouge-2 F1 Score: {}\".format(model_r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-generated summaries: Qualitative Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Generated Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num=30\n",
    "list(pd.Series(docs[doc_num].doc_sents).iloc[val_best_sents[doc_num]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Actual Head Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[doc_num].summary_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Generated Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num=300\n",
    "list(pd.Series(docs[doc_num].doc_sents).iloc[val_best_sents[doc_num]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Actual Head Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[doc_num].summary_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True average rouge score for our entire training set \n",
    "Based off true y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to drop null, or y's will not align\n",
    "docs2 = load_data(df['token_ops'].drop(null_y), df['token_heads'].drop(null_y))\n",
    "# Check that our document # and # of y labels match\n",
    "display(len(docs2))\n",
    "display(len(y_unproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the true summary labels from our unprocessed y_train\n",
    "y_true_combos = []\n",
    "for doc in y_unproc:\n",
    "    if doc != 'None':\n",
    "        y_p = doc.strip('(),').split(', ')\n",
    "        y_tpl = tuple(map(int, y_p))\n",
    "        y_true_combos.append(y_tpl)\n",
    "    else:\n",
    "        y_true_combos.append((9999999,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "true_scores = get_scores(y_true_combos, docs2)\n",
    "true_r1_score = find_avg_rouge(true_scores, \"rouge-1\", \"f\")\n",
    "true_r2_score = find_avg_rouge(true_scores, \"rouge-2\", \"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Rouge-1 F1 Score for entire dataset: {}\".format(true_r1_score))\n",
    "print(\"True Rouge-2 F1 Score for entire dataset: {}\".format(true_r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/model_img11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks on Results\n",
    "\n",
    "Our model is not too far off from the actual Rouge scores of our true labels, meaning that our model learned our labels relatively well. In order to further improve performance, we would need to improve our pre-processing step, where we better select our \"gold label\" sentences for extraction.\n",
    "\n",
    "Still, our model outperforms our baseline BERT, with Rouge-1 = 0.36 and Rouge-2 = 0.135\n",
    "\n",
    "Looking at our qualitative summary outputs, we also see that our extractive summaries are relatively intelligible and do appear to capture some of the important information from the original true summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
