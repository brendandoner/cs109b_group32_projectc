{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qilongxin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import json\n",
    "import lzma\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# pd.options.display.max_columns = 999\n",
    "# pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fucnction to remove \\n and HTML tags\n",
    "# function adapted from https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    text_divided = text.splitlines()\n",
    "    text_divided_clean = \" \".join(text_divided)\n",
    "    text_divided_clean = text_divided_clean.lower()\n",
    "    text_divided_clean = re.sub('\"','', text_divided_clean) # remove '\"'\n",
    "    text_divided_clean = re.sub(r\"'s\\b\",\"\",text_divided_clean) # remove ''s'\n",
    "    text_divided_clean = re.sub(\"[^a-zA-Z]\", \" \", text_divided_clean) # removes all strings that contains a non-letter\n",
    "    tokens = [w for w in text_divided_clean.split() if not w in stop_words] \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word (number of letters < 3)\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "\n",
    "# setting up tokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(state):\n",
    "    # reading json files\n",
    "    cases = []\n",
    "    with lzma.open(state + '/data/data.jsonl.xz', 'r') as jsonl_file:\n",
    "        for case in jsonl_file:\n",
    "            cases.append(json.loads(str(case, 'utf-8')))\n",
    "\n",
    "    df = pd.DataFrame(cases).sort_values('decision_date').reset_index(drop=True)\n",
    "    df['decision_date'] = pd.to_datetime(df['decision_date'])\n",
    "\n",
    "    # parsing data\n",
    "    storage = []\n",
    "    for i in range(df.shape[0]):\n",
    "        casebody_idx = df.columns.get_loc(\"casebody\")\n",
    "        judges = df.iloc[i,casebody_idx]['data']['judges']\n",
    "        attorneys = df.iloc[i,casebody_idx]['data']['attorneys']\n",
    "        headnotes = df.iloc[i,casebody_idx]['data']['head_matter']\n",
    "        if df.iloc[i,casebody_idx]['data']['opinions'] != []:\n",
    "            opinions = df.iloc[i,casebody_idx]['data']['opinions'][0]['text']\n",
    "\n",
    "        headnotes_clean = text_cleaner(headnotes)\n",
    "        opinions_clean = text_cleaner(opinions)\n",
    "\n",
    "        storage.append({'judges': judges,\n",
    "                        'attorneys': attorneys,\n",
    "                        'headnotes': headnotes_clean,\n",
    "                        'opinions': opinions_clean})\n",
    "    df_parsed = pd.DataFrame(storage)\n",
    "    df = df_parsed.merge(df, left_index=True, right_index=True)\n",
    "\n",
    "    # tokenizing headnotes and opinions\n",
    "    df['headnotes'] = df['headnotes'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    df['opinions'] = df['opinions'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    df['headnotes_num_tokens'] = [len(notes) for notes in df['headnotes']]\n",
    "    df['opinions_num_tokens'] = [len(opinions) for opinions in df['opinions']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Arkansas', 'Illinois', 'New Mexico', 'North Carolina']\n",
    "\n",
    "df_ar = get_data(states[0])\n",
    "df_il = get_data(states[1])\n",
    "df_nm = get_data(states[2])\n",
    "df_nc = get_data(states[3])\n",
    "\n",
    "dfs = [df_ar, df_il, df_nm, df_nc]\n",
    "\n",
    "dfs = [df_ar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,4, figsize=[18,8])\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "for c in range(len(states)):\n",
    "    df = dfs[c]\n",
    "    ax[0][c].plot(df.decision_date, df.headnotes_num_tokens, alpha=0.5)\n",
    "    ax[0][c].set_title('Headnotes' + states[c], fontsize=20)\n",
    "    ax[0][c].set_xlabel('Year of Decision', fontsize=16)\n",
    "    ax[0][c].set_ylabel('Token Count', fontsize=16)\n",
    "\n",
    "    ax[1][c].plot(df.decision_date, df.opinions_num_tokens)\n",
    "    ax[1][c].set_title('Opinions' + states[c], fontsize=20)\n",
    "    ax[1][c].set_xlabel('Year of Decision', fontsize=16)\n",
    "    ax[1][c].set_ylabel('Token Count', fontsize=16)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
